<!DOCTYPE html>
<html class="js no-flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths" lang="en"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	

	<title>Xiaoxu Meng</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="./files/modernizr-2.5.3.min.js.download"></script>

	<script src="./files/jquery.min.js.download"></script>
	<script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.7.2.min.js"><\/script>')</script>

	<script src="./files/spamspan.min.js.download"></script>
	<script src="./files/prettify.js.download"></script>

	<link rel="stylesheet" href="./files/social_widget.css">
	<link rel="stylesheet" href="./files/glyphicons.css">
	<link rel="stylesheet" href="./files/bootstrap.css">
	<link rel="stylesheet" href="./files/bootstrap-responsive.css">
	<link rel="stylesheet" href="./files/app.css">
	<!-- <link rel="stylesheet" href="./files/font-awesome.min.css"> -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<script type="text/javascript" src="./files/plugins.js.download"></script>
	<script type="text/javascript" src="./files/main.js.download"></script>
	<link rel="canonical" href="http://xiaoxumeng.github.io/">

	<!-- to toggle text -->
	<style type="text/css">
		a.toggle_text_link {
			cursor:pointer;
		}

		pre.invisible_text {
			display: none;
		}
	</style>
	<script language="javascript" type="text/javascript">
		function toggle(element) {
			if(element.style.display=="block") {
				element.style.display="none";
			} else {
				element.style.display="block";
			}
		}
	</script>
</head>

<body class="home page page-id-4 page-template-default top-navbar" data-gr-c-s-loaded="true">
  <!--[if lt IE 7]><div class="alert">Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</div><![endif]-->

    <header id="banner" class="navbar navbar-fixed-top" role="banner">
		<div class="navbar-inner">
			<div class="container">
				<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
					<!---<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				-->
				</a>
				<a class="brand" href="http://xiaoxumeng.github.io/">
					Xiaoxu Meng
				</a>
				<nav id="nav-main" class="nav-collapse" role="navigation">
					<ul class="nav">
						<li class="menu-home active active"><a href="http://xiaoxumeng.github.io/">Home</a></li>
					</ul>
				</nav>
			</div>
		</div>
	</header>

	<div id="wrap" class="container" role="document">
	<div id="content" class="row">
			<div id="main" class="span8" role="main">
				<div class="page-header">
					<h1>Xiaoxu Meng <img class="size-medium wp-image-712" style="margin-bottom: 5px;" src="./files/my_name.png" alt="Homepage" width="200" /></h1>
				</div>
				
				<p>
					<img class=" wp-image-697 alignleft dropshadow" style="margin: 0 1.5em 1em 0; border: 1px solid #222;" src="./files/photo_sanmiguel.jpeg" alt="Portrait photo of Xiaoxu Meng" width="200" height="288">

					I am currently employed as a Senior Researcher at Tencent America in Los Angeles. Prior to this, I earned my Ph.D. in Computer Science from the University of Maryland, College Park, advised by <a href="https://www.cs.umd.edu/~varshney/"> Dr. Amitabh Varshney</a>. 

					<br>
					<br>

					My research focuses on the intersection of computer graphics, computer vision, and virtual reality. I have contributed to various projects such as 3D reconstruction, differentiable rendering, foveated rendering in virtual reality, and denoising in real-time Monte Carlo path tracing. Additionally, I have held various roles within the academic community, including serving as a conference committee member for <a href="https://conferences.eg.org/egsr2023/">EGSR</a> and <a href="https://www.highperformancegraphics.org/2022/">HPG</a>, as well as the Student Volunteer Chair for <a href="http://ieeevr.org/2023/">IEEEVR</a>. I have also acted as a reviewer for several conferences and journals.
				
					<br>
					<br>

				<div class="alignbottom visible-desktop">
					<p>
						<a href="http://en.sjtu.edu.cn/">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/affiliation_logos/sjtu.png" alt="Shanghai Jiao Tong University" width="50" />
						</a>
								
						<a href="http://www.umd.edu/">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/affiliation_logos/umd.png.webp" alt="University of Maryland, College Park" width="50" />
						</a>
				
						<a href="https://about.google/">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/affiliation_logos/google.svg" alt="Google" width="50" />
						</a>

						<a href="https://tech.facebook.com/reality-labs/">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/affiliation_logos/frl.svg.png" alt="Facebook Reality Labs" width="50" />
						</a>
						<a href="https://www.tencent.com/en-us/index.html">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/affiliation_logos/Tencent_Logo.svg.png" alt="Tecent" width="200" />
						</a>
				
					</p>
				</div>

				<!--- News -->
				<hr style="clear: both;">
				
				<!--- Selected Publications -->
				<hr style="clear: both;">
				<h2> Publications </h2>
				<dl>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="http://geometrylearning.com/DreamUDF">
							<img class="dropshadow" alt="" src="./files/project_teaser/siga24_dreamudf.png" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="http://geometrylearning.com/DreamUDF">
							<strong>DreamUDF: Generating Unsigned Distance Fields from A Single Image</strong>
						</a>
						<br>
							<b>Yu-Tao Liu, Xuan Gao, Weikai Chen, Jie Yang, <b>Xiaoxu Meng</b>, Bo Yang, Lin Gao
						<br>
						<em> <b>SIGGRAPH Asia 2024</b></em>
						<br>
						<em>"a SDF-based differentiable rendering method that reconstructs arbitrary surfaces from multiview images"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="http://128.84.21.203/abs/2303.12012">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://xmeng525.github.io/xiaoxumeng.github.io/projects/cvpr23_neat">website</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/xmeng525/NeAT">code</a></li>
							<li><em class="fa fa-file"></em> <a href="https://youtu.be/GQNaW8GZOsM">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(neat);">abstract</a>
								<pre id="neat" class="invisible_text">Recent advances in diffusion models and neural implicit surfaces have shown promising progress in generating 3D models. However, existing generative frameworks are limited to closed surfaces, failing to cope with a wide range of commonly seen shapes that have open boundaries. In this work, we present DreamUDF, a novel framework for generating high-quality 3D objects with arbitrary topologies from a single image. To address the challenge of generating proper topology given sparse and ambiguous observations, we propose to incorporate both the data priors from a multi-view diffusion model and the geometry priors brought by an unsigned distance field (UDF) reconstructor. In particular, we leverage a joint framework that consists of 1) a generation module that produces a neural radiance field for photorealistic renderings from arbitrary views; and 2) a reconstruction module that distills the learnable radiance field into surfaces with arbitrary topologies. We further introduce a field coupler that bridges the radiance field and UDF under a novel optimization scheme. This allows the two modules to mutually boost each other during training. Extensive experiments and evaluations demonstrate that DreamUDF achieves high-quality reconstruction and robust 3D generation on both closed and open surfaces with arbitrary topologies, compared to the previous works.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://xmeng525.github.io/xiaoxumeng.github.io/projects/cvpr23_neat">
							<img class="dropshadow" alt="" src="./files/project_teaser/neat.jpg" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://xmeng525.github.io/xiaoxumeng.github.io/projects/cvpr23_neat">
							<strong>NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from Multi-view Images</strong>
						</a>
						<br>
							<b>Xiaoxu Meng</b>, Weikai Chen, Bo Yang
						<br>
						<em> <b>CVPR 2023</b></em>
						<br>
						<em>"a 3D generative model that can generate shapes with arbitrary topologies, including open surfaces"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="http://128.84.21.203/abs/2303.12012">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://xmeng525.github.io/xiaoxumeng.github.io/projects/cvpr23_neat">website</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/xmeng525/NeAT">code</a></li>
							<li><em class="fa fa-file"></em> <a href="https://youtu.be/GQNaW8GZOsM">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(neat);">abstract</a>
								<pre id="neat" class="invisible_text">Recent progress in neural implicit functions has set new state-of-the-art in reconstructing high-fidelity 3D shapes from a collection of images. However, these approaches are limited to closed surfaces as they require the surface to be represented by a signed distance field. In this paper, we propose NeAT, a new neural rendering framework that can learn implicit surfaces with arbitrary topologies from multi-view images. In particular, NeAT represents the 3D surface as a level set of a signed distance function (SDF) with a validity branch for estimating the surface existence probability at the query positions. We also develop a novel neural volume rendering method, which uses SDF and validity to calculate the volume opacity and avoids rendering points with low validity. NeAT supports easy field-to-mesh conversion using the classic Marching Cubes algorithm. Extensive experiments on DTU, MGN, and Deep Fashion 3D datasets indicate that our approach is able to faithfully reconstruct both watertight and non-watertight surfaces. In particular, NeAT significantly outperforms the state-of-the-art methods in the task of open surface reconstruction both quantitatively and qualitatively.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://openreview.net/pdf?id=Soadfc-JMeX">
							<img class="dropshadow" alt="" src="./files/project_teaser/hsdf.png" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://openreview.net/pdf?id=Soadfc-JMeX">
							<strong>NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering</strong>
						</a>
						<br>
						Yu-Tao Liu, Li Wang, Jie Yang, Weikai Chen, <b>Xiaoxu Meng</b>, Bo Yang, Lin Gao
						<br>
						<em> <b>CVPR 2023</b></em>
						<br>
						<em>"a UDF-based differentiable rendering method that reconstructs arbitrary surfaces from multiview images"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="http://geometrylearning.com/neudf/">website</a></li>
							<!-- <li><em class="fa fa-file"></em> <a href="https://openreview.net/pdf?id=Soadfc-JMeX">code</a></li> -->
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(neudf);">abstract</a>
								<pre id="neudf" class="invisible_text">Multi-view shape reconstruction has achieved impressive progresses thanks to the latest advances in neural implicit surface rendering. However, existing methods based on signed distance function (SDF) are limited to closed surfaces, failing to reconstruct a wide range of real-world objects that contain open-surface structures. In this work, we introduce a new neural rendering framework, coded NeUDF, that can reconstruct surfaces with arbitrary topologies solely from multi-view supervision. To gain the flexibility of representing arbitrary surfaces, NeUDF leverages the unsigned distance function (UDF) as surface representation. While a naive extension of SDF-based neural renderer cannot scale to UDF, we propose two new formulations of weight function specially tailored for UDF-based volume rendering. Furthermore, to cope with open surface rendering, where the in/out test is no longer valid, we present a dedicated normal regularization strategy to resolve the surface orientation ambiguity. We extensively evaluate our method over a number of challenging datasets, including DTU, MGN, and Deep Fashion 3D. Experimental results demonstrate that NeUDF can significantly outperform the state-of-the-art method in the task of multi-view surface reconstruction, especially for the complex shapes with open boundaries.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!-- <li><em class="fa fa-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li> -->
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4374184">
							<img class="dropshadow" alt="" src="./files/project_teaser/implicitpca.png" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4374184">
							<strong>IMPLICITPCA: Implicitly-Proxied Parametric Encoding for Collision-Aware Garment Reconstruction</strong>
						</a>
						<br>
						Lan Chen, Jie Yang, Hongbo Fu, <b>Xiaoxu Meng</b>, Weikai Chen, Bo Yang, Lin Gao
						<br>
						<em> <b>CVM 2023</b></em>
						<br>
						<em>"a parametric SDF network that closely couples parametric encoding with implicit functions, to enjoy the fine details brought by implicit reconstruction while maintaining correct open surfaces"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4374184">paper</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(IMPLICITPCA);">abstract</a>
								<pre id="IMPLICITPCA" class="invisible_text">The emerging remote collaboration in a virtual environment calls for the need for high-fidelity 3D human reconstruction from single image.To deal with the challenges of cloth details and topologies, parametric models are widely used as explicit priors. While they often lack of fine details from the image. Neural implicit approaches generate accurate details but are typically limited to closed surfaces.In addition, physically correct reconstructions, e.g. collision-free, is crucial but often ignored in prior works.We present ImplicitPCA, a parametric SDF network that closely couples parametric encoding with implicit functions, to enjoy the fine details brought by implicit reconstruction while maintaining correct open surfaces.We introduce a fast collision-aware regression network to ensure physically-correct estimation.During inference, an iterative routine is applied to align the garment to the 2D landmarks and fit with the collision-aware cloth SDF.The experiments on the public dataset and in-the-wild images demonstrate our outperformance.</pre>
							</li>
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://openreview.net/pdf?id=Soadfc-JMeX">
							<img class="dropshadow" alt="" src="./files/project_teaser/hsdf.png" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://openreview.net/pdf?id=Soadfc-JMeX">
							<strong>HSDF: Hybrid Sign and Distance Field for Modeling Surfaces with Arbitrary Topologies</strong>
						</a>
						<br>
						Li Wang, Jie Yang, Weikai Chen, <b>Xiaoxu Meng</b>, Bo Yang, Jintao Li, Lin Gao
						<br>
						<em> <b>NeurIPS 2022</b></em>
						<br>
						<em>"a learnable implicit representation for modeling both closed and open surfaces"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://openreview.net/pdf?id=Soadfc-JMeX">paper</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hsdf);">abstract</a>
								<pre id="hsdf" class="invisible_text">Neural implicit function based on signed distance field (SDF) has achieved impressive progress in reconstructing 3D models with high fidelity. However, such approaches can only represent closed shapes. Recent works based on unsigned distance function (UDF) are proposed to handle both watertight and open surfaces. Nonetheless, as UDF is signless, its direct output is limited to point cloud, which imposes an additional challenge on extracting high-quality meshes from discrete points. To address this issue, we present a new learnable implicit representation, coded HSDF, that connects the good ends of SDF and UDF. In particular, HSDF is able to represent arbitrary topologies containing both closed and open surfaces while being compatible with existing iso-surface extraction techniques for easy field-to-mesh conversion. In addition to predicting a UDF, we propose to learn an additional sign field via a simple classifier. Unlike traditional SDF, HSDF is able to locate the surface of interest before level surface extraction by generating surface points following NDF. We are then able to obtain open surfaces via an adaptive meshing approach that only instantiates regions containing surface into a polygon mesh. We also propose HSDF-Net, a dedicated learning framework that factorizes the learning of HSDF into two easier problems. Experiments on multiple datasets show that HSDF outperforms state-of-the-art techniques both qualitatively and quantitatively.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!-- <li><em class="fa fa-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li> -->
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="http://dalab.se.sjtu.edu.cn/www/home/?page_id=6440">
							<img class="dropshadow" alt="" src="./files/project_teaser/rmfr.webp" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="http://dalab.se.sjtu.edu.cn/www/home/?page_id=6440">
							<strong>Rectangular Mapping-based Foveated Rendering</strong>
						</a>
						<br>
						Jiannan Ye, Anqi Xie, Susmija Jabbireddy, Yunchuan Li, Xubo Yang, <b>Xiaoxu Meng</b>
						<br>
						<em> <b>IEEEVR 2022</b></em>
						<br>
						<em>"a simple yet effective implementation of foveated rendering framework"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="http://dalab.se.sjtu.edu.cn/www/home/wp-content/uploads/2022/01/vr22b-sub1543-cam-i6.pdf">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="http://dalab.se.sjtu.edu.cn/www/home/?page_id=6440">website</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/xmeng525/Rectangular-Mapping-based-Foveated-Rendering">code</a></li>
							<li><em class="fa fa-file"></em> <a href="https://youtu.be/t8g5EoUZDIM">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(rmfr);">abstract</a>
								<pre id="rmfr" class="invisible_text">With the speedy increase of display resolution and the demand for interactive frame rate, rendering acceleration is becoming more critical for a wide range of virtual reality applications. Foveated rendering addresses this challenge by rendering with a non-uniform resolution for the display. Motivated by the non-linear optical lens equation, we present rectangular mapping-based foveated rendering (RMFR), a simple yet effective implementation of foveated rendering framework. RMFR supports varying level of foveation according to the eccentricity and the scene complexity. Compared with traditional foveated rendering methods, rectangular mapping-based foveated rendering provides a superior level of perceived visual quality while consuming minimal rendering cost.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!-- <li><em class="fa fa-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li> -->
						</ul>
					</div>
				</div>


				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://sites.google.com/view/bilateral-grid-denoising">
							<img class="dropshadow" alt="" src="./files/project_teaser/bilateral_grid.webp" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://sites.google.com/view/bilateral-grid-denoising">
							<strong>Real-time Monte Carlo Denoising with the Neural Bilateral Grid</strong>
						</a>
						<br>
						<b>Xiaoxu Meng</b>, Quan Zheng, Amitabh Varshney, Gurprit Singh, and Matthias Zwicker
						<br>
						<em> <b>EGSR 2020</b></em>
						<br>
						<em>"an efficient neural network architecture learns to denoise noisy renderings in a data-dependent, bilateral space"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://xiaoxumeng.com/Documents/Meng_EGSR_2020/Real_time_Monte_Carlo_Denoising_with_the_Neural_Bilateral_Grid_EGSR.pdf">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://sites.google.com/view/bilateral-grid-denoising">website</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/xmeng525/RealTimeDenoisingNeuralBilateralGrid">code</a></li>
							<li><em class="fa fa-file"></em> <a href="https://youtu.be/9PVR1-GTt6g">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(rmcd);">abstract</a>
								<pre id="rmcd" class="invisible_text">Real-time denoising for Monte Carlo rendering remains a critical challenge with regard to the demanding requirements of both high fidelity and low computation time. In this paper, we propose a novel and practical deep learning approach to robustly denoise Monte Carlo images rendered at sampling rates as low as a single sample per pixel (1-spp). This causes severe noise, and previous techniques strongly compromise final quality to maintain real-time denoising speed. We develop an efficient convolutional neural network architecture to learn to denoise noisy inputs in a data-dependent, bilateral space. Our neural network learns to generate a guide image for first splatting noisy data into the grid, and then slicing it to read out the denoised data. To seamlessly integrate bilateral grids into our trainable denoising pipeline, we leverage a differentiable bilateral grid, called neural bilateral grid, which enables end-to-end training. In addition, we also show how we can further improve denoising quality using a hierarchy of multi-scale bilateral grids. Our experimental results demonstrate that this approach can robustly denoise 1-spp noisy input images at real-time frame rates (a few milliseconds per frame). At such low sampling rates, our approach outperforms state-of-the-art techniques based on kernel prediction networks both in terms of quality and speed, and it leads to significantly improved quality compared to the state-of-the-art feature regression technique.</pre>
							</li>
						</ul>
					</div>
				</div>


				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://foveation.umiacs.umd.edu/3D_KFR">
							<img class="dropshadow" alt="" src="./files/project_teaser/3dkfr.webp" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://foveation.umiacs.umd.edu/3D_KFR">
							<strong>3D-Kernel Foveated Rendering for Light Fields</strong>
						</a>
						<br>
						<b>Xiaoxu Meng</b>, Ruofei Du, Joseph F. JaJa, Amitabh Varshney
						<br>
						<em> <b>TVCG</b></em>
						<br>
						<em>"a perceptual model for foveated light fields by extending the KFR for the rendering of 3D meshes"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://d63055d3-a3ce-4372-9d02-f6594085f672.filesusr.com/ugd/c0ebd5_9c32a9bacb1b416f90c0652c92ca6519.pdf">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://foveation.umiacs.umd.edu/3D_KFR">website</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/xmeng525/3DKernelFoveatedRenderingForLightFields">code</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=ZEUU9yr2H4Q">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(threedkfr);">abstract</a>
								<pre id="threedkfr" class="invisible_text">Light fields capture both the spatial and angular rays, thus enabling free-viewpoint rendering and custom selection of the focal plane. Scientists can interactively explore pre-recorded microscopic light fields of organs, microbes, and neurons using virtual reality headsets. However, rendering high-resolution light fields at interactive frame rates requires a very high rate of texture sampling, which is challenging as the resolutions of light fields and displays continue to increase. In this article, we present an efficient algorithm to visualize 4D light fields with 3D-kernel foveated rendering (3D-KFR). The 3D-KFR scheme coupled with eye-tracking has the potential to accelerate the rendering of 4D depth-cued light fields dramatically. We have developed a perceptual model for foveated light fields by extending the KFR for the rendering of 3D meshes. On datasets of high-resolution microscopic light fields, we observe 3.47×-7.28× speedup in light field rendering with minimal perceptual loss of detail. We envision that 3D-KFR will reconcile the mutually conflicting goals of visual fidelity and rendering speed for interactive visualization of light fields.</pre>
							</li>
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://duruofei.com/papers/Meng_Eye-Dominance-GuidedFoveatedRendering_TVCG2020.pdf">
							<img class="dropshadow" alt="" src="./files/project_teaser/efr.webp" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://duruofei.com/papers/Meng_Eye-Dominance-GuidedFoveatedRendering_TVCG2020.pdf">
							<strong>Eye-dominance-guided Foveated Rendering</strong>
						</a>
						<br>
						<b>Xiaoxu Meng</b>, Ruofei Du, Amitabh Varshney
						<br>
						<em> <b>IEEEVR 2020 (Accepted as IEEE TVCG paper)</b></em>
						<br>
						<em>"renders the scene with higher detail for the dominant eye than the non-dominant eye"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://duruofei.com/papers/Meng_Eye-Dominance-GuidedFoveatedRendering_TVCG2020.pdf">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://youtu.be/qik08dMy9F4">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(efr);">abstract</a>
								<pre id="efr" class="invisible_text">Optimizing rendering performance is critical for a wide variety of virtual reality (VR) applications. Foveated rendering is emerging as an indispensable technique for reconciling interactive frame rates with ever-higher head-mounted display resolutions. Here, we present a simple yet effective technique for further reducing the cost of foveated rendering by leveraging ocular dominance - the tendency of the human visual system to prefer scene perception from one eye over the other. Our new approach, eye-dominance-guided foveated rendering (EFR), renders the scene at a lower foveation level (with higher detail) for the dominant eye than the non-dominant eye. Compared with traditional foveated rendering, EFR can be expected to provide superior rendering performance while preserving the same level of perceived visual quality.</pre>
							</li>
						</ul>
					</div>
				</div>

				<div class="row-fluid">
					<div class="span4" style="margin-bottom: 2em;">
						<a href="https://duruofei.com/papers/Meng_KernelFoveatedRendering_I3D2018.pdf">
							<img class="dropshadow" alt="" src="./files/project_teaser/kfr.webp" width="300" height="150">
						</a>
					</div>
					<div class="span8">
						<a href="https://duruofei.com/papers/Meng_KernelFoveatedRendering_I3D2018.pdf">
							<strong>Kernel Foveated Rendering</strong>
						</a>
						<br>
						<b>Xiaoxu Meng</b>, Ruofei Du, Matthias Zwicker, and Amitabh Varshney
						<br>
						<em> <b>I3D 2018 (Accepted as journal paper)</b></em>
						<br>
						<em>"renders the scene with higher detail for the dominant eye than the non-dominant eye"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://duruofei.com/papers/Meng_KernelFoveatedRendering_I3D2018.pdf">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/xmeng525/KernelFoveatedRendering_Unity">code</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=VOpC-xEaB-Q&feature=youtu.be">video</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(kfr);">abstract</a>
								<pre id="kfr" class="invisible_text">Foveated rendering coupled with eye-tracking has the potential to dramatically accelerate interactive 3D graphics with minimal loss of perceptual detail. In this paper, we parameterize foveated rendering by embedding polynomial kernel functions in the classic log-polar mapping. Our GPU-driven technique uses closed-form, parameterized foveation that mimics the distribution of photoreceptors in the human retina. We present a simple two-pass kernel foveated rendering (KFR) pipeline that maps well onto modern GPUs. In the first pass, we compute the kernel log-polar transformation and render to a reduced-resolution buffer. In the second pass, we carry out the inverse-log-polar transformation with anti-aliasing to map the reduced-resolution rendering to the full-resolution screen. We have carried out pilot and formal user studies to empirically identify the KFR parameters. We observe a 2.8X -- 3.2X speedup in rendering on 4K UHD (2160p) displays with minimal perceptual loss of detail. The relevance of eye-tracking-guided kernel foveated rendering can only increase as the anticipated rise of display resolution makes it ever more difficult to resolve the mutually conflicting goals of interactive rendering and perceptual realism.</pre>
							</li>
						</ul>
					</div>
				</div>

				</dl>
				</dl>
			

			</div><!-- /#main -->

			<aside id="sidebar" class="span4" role="complementary">
				<div class="well">
					<section id="text-2" class="widget-1 widget-first widget widget_text">
						<div class="widget-inner">
							<h3>Contact info</h3>
							<div class="textwidget"><h6>Email</h6>
								<a class="email" href="mailto:xmeng525@terpmail.umd.edu">x{my_last_name}525 at terpmail.umd.edu</a>
								<h6>Address</h6>
								<i>
									Los Angeles, CA, U.S.A
								</i>
							</div>							
						</div>
					</section>
				
					<br>
					<section id="text-2" class="widget-1 widget-first widget widget_text">
						<div class="widget-inner">
							<h3> <a href="./files/CV_XiaoxuMeng.pdf"> Curriculum Vitae </a></h3>
						</div>
					</section>
					<br>
		
					<section id="social-widget-2" class="widget-3 widget Social_Widget">
						<div class="widget-inner">
							<h3>Follow me</h3>
							<div class="socialmedia-buttons smw_left">
								<a href="https://scholar.google.com/citations?user=Ko_--SMAAAAJ&hl=en" rel="publisher" target="_blank">
									<img width="32" height="32" src="./files/scholar.png" alt="Follow me on Google Scholar" title="Follow me on Google Scholar" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>
								<a href="https://github.com/xmeng525" rel="nofollow" target="_blank">
									<img width="32" height="32" src="./files/github.png" alt="Follow me on GitHub" title="Follow me on GitHub" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>					
								<a href="https://www.linkedin.com/in/xiaoxu-meng-682b54128/" rel="nofollow" target="_blank">
									<img width="32" height="32" src="./files/linkedin.png" alt="Follow me on LinkedIn" title="Follow me on LinkedIn " style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>

								<!-- site credits -->
								<br><br>
								<em class="fa fa-credit-card"></em> <a class="toggle_text_link" onclick="javascript:toggle(site_credits);">Site Credits</a>
								<pre id="site_credits" class="invisible_text">
This site was built using <a href="http://twitter.github.io/bootstrap/">Bootstrap</a>, a front-end framework for web development. <br /><br />The style of this site is inspired by <a href="http://richardt.name/">Dr. Christian Richardt</a>'s and <a href="http://herohuyongtao.github.io/">Dr. Yongtao Hu</a>'s personal website and <a href="http://chenweikai.github.io">Dr. Weikai Chen</a>'s personal website.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								
							</div>
						</div>
					</section>
				</div>
			</aside><!-- /#sidebar -->
		</div><!-- /#content -->
	</div><!-- /#wrap -->

	<footer id="content-info" class="container" role="contentinfo">
		<table width="100%">
			<tbody><tr>
				<td>
					<p class="copy"><small>© 2023 Xiaoxu Meng</small></p>
				</td>
				<td style="text-align:right">
					<script type="text/javascript" src="//rf.revolvermaps.com/0/0/4.js?i=57g8g4mig5b&amp;m=0&amp;h=128&amp;c=ff0000&amp;r=0" async="async"></script>
				
				</td>
			</tr>
		</tbody></table>
	</footer>


<div id="cntvlive2-is-installed"></div></body></html>